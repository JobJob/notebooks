{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage in Julia\n",
    "\n",
    "Some simple examples to show how easy it is to do computations on your GPU from Julia. These examples all require an NVIDIA GPU with CUDA installed. There is support also for non-NVIDIA GPUs via OpenCL using the [CLArrays Package](https://github.com/JuliaGPU/CLArrays.jl), though I haven't tested it out.\n",
    "\n",
    "The packages used here are:\n",
    "\n",
    "[CUArrays](https://github.com/JuliaGPU/CUArrays.jl): Provides GPU arrays and supports standard operations on them - see the README for more details. Simple to use, no prior knowledge of using, or programming GPUs is really needed.\n",
    "\n",
    "[CUDAnative](https://github.com/JuliaGPU/CUArrays.jl): Allows you to write your own custom GPU kernels (programs that run in parallel on the GPU) using Julia. Further reading on the advantages of using Julia for this: http://mikeinnes.github.io/2017/08/24/cudanative.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below to install CuArrays\n",
    "\n",
    "# using Pkg; Pkg.add(\"CuArrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CuArrays\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply two 1000x1000 element matrices on the cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20.277 ms (2 allocations: 7.63 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       " 256.592  248.702  245.313  244.702  …  249.644  245.923  243.769  253.103\n",
       " 261.498  257.975  253.463  255.377     254.744  254.901  248.419  262.017\n",
       " 253.64   242.134  241.193  249.984     248.066  240.754  244.385  252.285\n",
       " 256.114  245.055  244.443  249.676     254.11   244.597  239.771  254.641\n",
       " 259.852  249.964  249.099  254.542     250.815  246.535  247.636  257.095\n",
       " 252.457  246.472  243.208  247.211  …  248.711  242.678  247.677  254.618\n",
       " 255.461  242.733  241.797  252.106     245.892  244.96   249.522  257.296\n",
       " 254.716  248.481  251.584  251.931     248.815  244.849  247.925  253.883\n",
       " 266.108  256.503  253.956  252.816     255.772  250.646  253.961  261.284\n",
       " 249.175  244.057  237.917  250.194     242.806  238.042  241.543  246.982\n",
       " 250.288  248.48   241.844  249.041  …  246.038  243.617  240.9    253.655\n",
       " 252.601  249.351  253.049  253.525     247.938  244.167  247.965  253.656\n",
       " 260.055  254.522  252.469  252.417     259.704  254.414  247.356  262.053\n",
       "   ⋮                                 ⋱                                    \n",
       " 259.272  245.517  243.361  248.528     251.174  246.291  242.568  252.677\n",
       " 254.034  250.5    246.125  245.821     254.832  247.188  250.715  252.625\n",
       " 263.27   255.975  252.488  263.633  …  256.062  255.389  249.945  267.339\n",
       " 260.684  248.144  249.305  252.896     256.232  248.629  252.293  256.163\n",
       " 252.847  244.166  242.289  243.387     244.674  245.712  241.69   254.622\n",
       " 257.027  247.368  242.001  242.461     247.33   244.328  244.99   251.203\n",
       " 254.424  250.214  247.021  252.227     252.438  248.331  245.604  253.349\n",
       " 270.115  259.531  253.019  257.098  …  257.013  255.721  259.134  263.752\n",
       " 249.465  239.63   238.642  243.17      245.539  240.057  243.653  246.326\n",
       " 247.719  242.814  244.821  243.688     245.669  246.749  243.216  249.835\n",
       " 251.644  241.165  241.296  246.058     244.944  240.019  240.944  248.779\n",
       " 257.85   252.781  246.908  250.36      249.811  249.466  244.524  255.187"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use const here because [non-const globals are slow](https://docs.julialang.org/en/v1/manual/performance-tips/index.html#Avoid-global-variables-1)\n",
    "const dim = 1000\n",
    "const ac = rand(dim, dim)\n",
    "const bc = rand(dim, dim)\n",
    "\n",
    "@btime ac*bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10.225 μs (11 allocations: 400 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32,2}:\n",
       " 256.592  248.702  245.313  244.702  …  249.644  245.923  243.769  253.103\n",
       " 261.498  257.975  253.463  255.377     254.744  254.901  248.419  262.017\n",
       " 253.64   242.134  241.193  249.984     248.066  240.754  244.385  252.285\n",
       " 256.114  245.055  244.443  249.676     254.11   244.597  239.771  254.641\n",
       " 259.852  249.964  249.099  254.542     250.815  246.535  247.636  257.096\n",
       " 252.458  246.472  243.209  247.211  …  248.711  242.678  247.677  254.618\n",
       " 255.461  242.733  241.797  252.106     245.892  244.96   249.522  257.296\n",
       " 254.716  248.481  251.584  251.931     248.815  244.848  247.925  253.883\n",
       " 266.108  256.503  253.956  252.816     255.772  250.646  253.961  261.284\n",
       " 249.175  244.057  237.917  250.194     242.806  238.042  241.543  246.981\n",
       " 250.288  248.48   241.843  249.041  …  246.039  243.617  240.9    253.655\n",
       " 252.601  249.351  253.049  253.524     247.938  244.167  247.965  253.656\n",
       " 260.055  254.522  252.469  252.417     259.704  254.413  247.356  262.053\n",
       "   ⋮                                 ⋱                                    \n",
       " 259.272  245.517  243.361  248.528     251.175  246.291  242.568  252.677\n",
       " 254.033  250.5    246.125  245.821     254.832  247.188  250.715  252.625\n",
       " 263.27   255.975  252.488  263.633  …  256.062  255.389  249.945  267.339\n",
       " 260.684  248.144  249.305  252.896     256.232  248.629  252.293  256.163\n",
       " 252.847  244.166  242.289  243.387     244.675  245.711  241.691  254.623\n",
       " 257.027  247.368  242.001  242.461     247.33   244.328  244.99   251.203\n",
       " 254.425  250.215  247.021  252.227     252.438  248.331  245.604  253.349\n",
       " 270.115  259.531  253.019  257.098  …  257.012  255.721  259.134  263.752\n",
       " 249.465  239.63   238.642  243.17      245.539  240.057  243.653  246.326\n",
       " 247.718  242.814  244.821  243.687     245.669  246.749  243.216  249.835\n",
       " 251.644  241.165  241.296  246.058     244.944  240.02   240.944  248.779\n",
       " 257.85   252.781  246.908  250.36      249.811  249.466  244.524  255.187"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const ag = cu(ac)\n",
    "const bg = cu(bc)\n",
    "@btime ag*bg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Write our own kernel to add two matrices together in parallel on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below to install CUDAnative.jl\n",
    "\n",
    "# using Pkg; Pkg.add(\"CUDAnative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a known test array so we can eyeball our results to make sure they look correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testarr (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function testarr(shape...)\n",
    "    len = prod(shape) # length (number of elements) is the product of the dimensions\n",
    "    return collect(reshape(1:len, shape)) # reshape it to \"shape\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two NxN arrays\n",
    "const N = 1024\n",
    "const xc = testarr(N, N)\n",
    "const yc = testarr(N, N)\n",
    "const zc = zeros(Int64, size(xc)); # an array to store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.653 ms (2 allocations: 8.00 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024×1024 Array{Int64,2}:\n",
       "    2  2050  4098  6146   8194  10242  …  2088962  2091010  2093058  2095106\n",
       "    4  2052  4100  6148   8196  10244     2088964  2091012  2093060  2095108\n",
       "    6  2054  4102  6150   8198  10246     2088966  2091014  2093062  2095110\n",
       "    8  2056  4104  6152   8200  10248     2088968  2091016  2093064  2095112\n",
       "   10  2058  4106  6154   8202  10250     2088970  2091018  2093066  2095114\n",
       "   12  2060  4108  6156   8204  10252  …  2088972  2091020  2093068  2095116\n",
       "   14  2062  4110  6158   8206  10254     2088974  2091022  2093070  2095118\n",
       "   16  2064  4112  6160   8208  10256     2088976  2091024  2093072  2095120\n",
       "   18  2066  4114  6162   8210  10258     2088978  2091026  2093074  2095122\n",
       "   20  2068  4116  6164   8212  10260     2088980  2091028  2093076  2095124\n",
       "   22  2070  4118  6166   8214  10262  …  2088982  2091030  2093078  2095126\n",
       "   24  2072  4120  6168   8216  10264     2088984  2091032  2093080  2095128\n",
       "   26  2074  4122  6170   8218  10266     2088986  2091034  2093082  2095130\n",
       "    ⋮                               ⋮  ⋱        ⋮                           \n",
       " 2026  4074  6122  8170  10218  12266     2090986  2093034  2095082  2097130\n",
       " 2028  4076  6124  8172  10220  12268     2090988  2093036  2095084  2097132\n",
       " 2030  4078  6126  8174  10222  12270     2090990  2093038  2095086  2097134\n",
       " 2032  4080  6128  8176  10224  12272  …  2090992  2093040  2095088  2097136\n",
       " 2034  4082  6130  8178  10226  12274     2090994  2093042  2095090  2097138\n",
       " 2036  4084  6132  8180  10228  12276     2090996  2093044  2095092  2097140\n",
       " 2038  4086  6134  8182  10230  12278     2090998  2093046  2095094  2097142\n",
       " 2040  4088  6136  8184  10232  12280     2091000  2093048  2095096  2097144\n",
       " 2042  4090  6138  8186  10234  12282  …  2091002  2093050  2095098  2097146\n",
       " 2044  4092  6140  8188  10236  12284     2091004  2093052  2095100  2097148\n",
       " 2046  4094  6142  8190  10238  12286     2091006  2093054  2095102  2097150\n",
       " 2048  4096  6144  8192  10240  12288     2091008  2093056  2095104  2097152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zc .= xc + yc # non-allocating elementwise sum\n",
    "@btime zc .= xc + yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.883 μs (33 allocations: 1.23 KiB)\n"
     ]
    }
   ],
   "source": [
    "using CuArrays, CUDAnative\n",
    "\n",
    "xg, yg = CuArray(xc), CuArray(yc)\n",
    "zg = CuArray(zeros(Int64, size(zc))) # an array to store the result\n",
    "\n",
    "\"\"\"\n",
    "A very simple kernel to add two arrays\n",
    "\"\"\"\n",
    "function kernel_vadd(out, a, b)\n",
    "    # get the index into the arrays that this GPU processor will work on\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    \n",
    "    # add the two elements and store the result\n",
    "    out[i] = a[i] + b[i]\n",
    "    \n",
    "    # kernels must return `nothing`\n",
    "    return\n",
    "end\n",
    "\n",
    "# launch (i.e. run) the GPU kernel with the same number of (threads, blocks) as (rows, columns) in our arrays\n",
    "@btime @cuda threads=N blocks=N kernel_vadd(zg, xg, yg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024×1024 CuArray{Int64,2}:\n",
       "    2  2050  4098  6146   8194  10242  …  2088962  2091010  2093058  2095106\n",
       "    4  2052  4100  6148   8196  10244     2088964  2091012  2093060  2095108\n",
       "    6  2054  4102  6150   8198  10246     2088966  2091014  2093062  2095110\n",
       "    8  2056  4104  6152   8200  10248     2088968  2091016  2093064  2095112\n",
       "   10  2058  4106  6154   8202  10250     2088970  2091018  2093066  2095114\n",
       "   12  2060  4108  6156   8204  10252  …  2088972  2091020  2093068  2095116\n",
       "   14  2062  4110  6158   8206  10254     2088974  2091022  2093070  2095118\n",
       "   16  2064  4112  6160   8208  10256     2088976  2091024  2093072  2095120\n",
       "   18  2066  4114  6162   8210  10258     2088978  2091026  2093074  2095122\n",
       "   20  2068  4116  6164   8212  10260     2088980  2091028  2093076  2095124\n",
       "   22  2070  4118  6166   8214  10262  …  2088982  2091030  2093078  2095126\n",
       "   24  2072  4120  6168   8216  10264     2088984  2091032  2093080  2095128\n",
       "   26  2074  4122  6170   8218  10266     2088986  2091034  2093082  2095130\n",
       "    ⋮                               ⋮  ⋱        ⋮                           \n",
       " 2026  4074  6122  8170  10218  12266     2090986  2093034  2095082  2097130\n",
       " 2028  4076  6124  8172  10220  12268     2090988  2093036  2095084  2097132\n",
       " 2030  4078  6126  8174  10222  12270     2090990  2093038  2095086  2097134\n",
       " 2032  4080  6128  8176  10224  12272  …  2090992  2093040  2095088  2097136\n",
       " 2034  4082  6130  8178  10226  12274     2090994  2093042  2095090  2097138\n",
       " 2036  4084  6132  8180  10228  12276     2090996  2093044  2095092  2097140\n",
       " 2038  4086  6134  8182  10230  12278     2090998  2093046  2095094  2097142\n",
       " 2040  4088  6136  8184  10232  12280     2091000  2093048  2095096  2097144\n",
       " 2042  4090  6138  8186  10234  12282  …  2091002  2093050  2095098  2097146\n",
       " 2044  4092  6140  8188  10236  12284     2091004  2093052  2095100  2097148\n",
       " 2046  4094  6142  8190  10238  12286     2091006  2093054  2095102  2097150\n",
       " 2048  4096  6144  8192  10240  12288     2091008  2093056  2095104  2097152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: arrays are equal\n",
      "└ @ Main In[10]:2\n"
     ]
    }
   ],
   "source": [
    "if zc == zg\n",
    "    @info \"arrays are equal\"\n",
    "else\n",
    "    @warn \"something went wrong: arrays are NOT equal\"\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
